---
- name: Apply network plugin
  hosts: vm_host
  run_once: true
  vars_files:
    - vars/k8s_cluster.yml
  tasks:
    - name: Configure Calico
      block:
        - name: Download Calico manifest.
          ansible.builtin.get_url:
            url: "{{ item.url }}"
            dest: "{{ item.name }}"
            mode: '0664'
          loop:
            - name: "{{ playbook_dir }}/../../calico/kustomize/operator/base/tigera-operator.yaml"
              url: "{{ cni_plugins.calico.calico_operator }}"
            - name: "{{ playbook_dir }}/../../calico/kustomize/custom-resources/base/custom-resources.yaml"
              url: "{{ cni_plugins.calico.calico_crd }}"
            # - name: "{{ playbook_dir }}/../../calico/kustomize/operator/base/csi-driver.yaml"
            #   url: "{{ cni_plugins.calico.csi_driver }}"
            - name: "{{ playbook_dir }}/../../istio/kustomize/istiod/base/istio-app-layer-policy-envoy-v3.yaml"
              url: "{{ cni_plugins.calico.istio_policy }}"
              

        - name: Kustomize build operator manifests.
          ansible.builtin.shell:
            cmd: kustomize build
            chdir: "{{ playbook_dir }}/../../calico/kustomize/operator/"
          register: myoutput
        # - debug: var=myoutput.stdout

        - name: Kustomize copy operator resources.
          ansible.builtin.copy :
            content: "{{ myoutput.stdout }}"
            dest: "{{ playbook_dir }}/../../calico/kustomize/operator/resources/resources.yaml"
            mode: "0644"

        - name: Apply operator manifests to the cluster.
          kubernetes.core.k8s:
            state: present
            src: "{{ playbook_dir }}/../../calico/kustomize/operator/resources/resources.yaml"
            kubeconfig: "{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig"
            wait: true

        - name: Setting kustomize custom-resources patch templates
          ansible.builtin.template:
            src: "{{ item }}"
            dest: "{{ playbook_dir }}/../../calico/kustomize/custom-resources/patches/{{ item | basename | regex_replace('\\.j2$', '') }}"
            mode: "0644"
          with_fileglob:
            - "{{ playbook_dir }}/../../calico/kustomize/custom-resources/patches/templates/*.j2"

        - name: Kustomize build custom-resources manifests.
          ansible.builtin.shell:
            cmd: kustomize build
            chdir: "{{ playbook_dir }}/../../calico/kustomize/custom-resources/"
          register: myoutput
        # - debug: var=myoutput.stdout

        - name: Kustomize copy custom-resources resources.
          ansible.builtin.copy :
            content: "{{ myoutput.stdout }}"
            dest: "{{ playbook_dir }}/../../calico/kustomize/custom-resources/resources/resources.yaml"
            mode: "0644"

        - name: Apply calico manifests to the cluster.
          kubernetes.core.k8s:
            state: present
            src: "{{ playbook_dir }}/../../calico/kustomize/custom-resources/resources/resources.yaml"
            kubeconfig: "{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig"
            wait: true

        - name: Wait for FelixConfiguration to exist
          ansible.builtin.shell: 
            cmd: | 
              while ! \
                kubectl get FelixConfiguration/default \
                --kubeconfig='{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig';
              do sleep 1;
              done
          register: myoutput

        - debug: var=myoutput.stdout_lines

        - name: Patch FelixConfiguration
          kubernetes.core.k8s:
            state: patched
            kubeconfig: "{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig"
            kind: FelixConfiguration
            name: default
            definition:
              spec:
                policySyncPathPrefix: /var/run/nodeagent
                prometheusMetricsEnabled: true

      when: k8s.network.cni_plugin == 'calico'

    - name: Configure Cilium
      block:
        - name: Add helm chart repository for Cilium
          kubernetes.core.helm_repository:
            name: "{{ item.name }}"
            repo_url: "{{ item.repo_url }}"
          loop:
            - name: "{{ cni_plugins.cilium.chart.name }}"
              repo_url: "{{ cni_plugins.cilium.chart.url }}"

        - name: Ensure Cilium helm chart is installed
          kubernetes.core.helm:
            name: cilium
            chart_ref: "{{ cni_plugins.cilium.chart.ref }}"
            kubeconfig: "{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig"
            release_namespace: kube-system
            update_repo_cache: true
            values:
              ipam:
                mode: kubernetes
            wait: true
      when: k8s.network.cni_plugin == 'cilium'

    - name: Configure flannel
      block:
        - name: Download flannel manifest
          ansible.builtin.get_url:
            url: "{{ cni_plugins.flannel.flannel_repo }}"
            dest: /tmp/{{ k8s.cluster_name }}/kube-flannel.yaml
            mode: 0755

        - name: Patch kube-flannel to use host-gw instead of vxlan
          ansible.builtin.replace:
            path: /tmp/{{ k8s.cluster_name }}/kube-flannel.yaml
            regexp: 'vxlan'
            replace: 'host-gw'

        - name: Apply flannel manifests to the cluster.
          kubernetes.core.k8s:
            state: present
            src: /tmp/{{ k8s.cluster_name }}/kube-flannel.yaml
            kubeconfig: "{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig"
            wait: true
      when: k8s.network.cni_plugin == 'flannel'

    - name: Restart core-dns for valid pod anti-affinity
      ansible.builtin.shell: 
        cmd: | 
          kubectl rollout restart -n kube-system deployment coredns \
          --kubeconfig='{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig'
      register: myoutput

    - debug: var=myoutput.stdout_lines

    - name: Wait for core-dns pods to be up and running
      kubernetes.core.k8s:
        state: present
        api_version: v1
        kind: Deployment
        namespace: kube-system
        name: coredns
        kubeconfig: "{{ workspace_directory.base_path }}/clusters/{{ k8s.cluster_name }}/admin.kubeconfig"
        wait: true
